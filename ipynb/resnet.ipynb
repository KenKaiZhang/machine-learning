{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of ResNet introduced in the paper **Deep Residual Learning for Image Recognition** found [here](https://arxiv.org/abs/1512.03385). This notebook is just a way for me to have an understanding of what I did in the code found in `resnet.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet is basically consists of 5 main layers where 4 of them are residual blocks that have almost identical architecture. We can build these individual block in a reusable component (`class`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=None, stride=1):\n",
    "        super().__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv3d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        donwsample = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            donwsample = self.downsample(donwsample)\n",
    "\n",
    "        x += donwsample\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few key things to note.\n",
    "\n",
    "`in_channels` : the number of channels going into the layer (block in this case as well).\n",
    "\n",
    "`out_channels` : the number of channels that will be produced by the layer \n",
    "\n",
    "Starting from `conv2d`, we input the `out_channel` as `in_channel` because `conv2` operates on the results of `conv1` and since `conv1` returns a feature map with `out_channel` channels, we pass it along as such.\n",
    "\n",
    "`conv2` has its stride not set to 1 for\n",
    "- faster convolution given the larger kernel\n",
    "- reduce computational load for the next layer (reduced spatial dimension)\n",
    "\n",
    "Regarding the `if` statement that checks if `downsample` is not `None`, this is done to ensure the dimensions of the input will be the same as the dimensions of the output because if it's not, it will require the input to go through a`downsample` function before converging with the output with `+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_res_blocks, out_channels, stride):\n",
    "        downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels * 4:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels*4)\n",
    "            )\n",
    "        \n",
    "        layers.append(block(self.in_channels, out_channels, downsample, stride))\n",
    "        self.in_channels = out_channels * 4\n",
    "\n",
    "        for n in range(num_res_blocks - 1):\n",
    "            layers.append(block. self.in_channels, out_channels)\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting part here is `_make_layer` (everything is else is basically stripped from the diagram).\n",
    "\n",
    "We assume that downsampling is only needed when...\n",
    "1. `stride != 1` : there is a spatial dimensionality change\n",
    "2. `self.in_channels != out_channels * 4` : the input does not match the output spatial dimension\n",
    "\n",
    "The architecture has it such that each layer has different number of residual blocks, so with `layers`, we can pass in the desired number of residual blocks per layer (index) for our net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(img_channels=3, num_classes=1000):\n",
    "    return ResNet(Block, [3, 4, 6, 3], img_channels, num_classes)\n",
    "\n",
    "def ResNet101(img_channels=3, num_classes=1000):\n",
    "    return ResNet(Block, [3, 4, 23, 3], img_channels, num_classes)\n",
    "\n",
    "def ResNet152(img_channels=3, num_classes=1000):\n",
    "    return ResNet(Block, [3, 8, 36, 3], img_channels, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
