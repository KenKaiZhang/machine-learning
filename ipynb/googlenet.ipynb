{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of GoogLeNet introduced in **Going deeper with convolutions** found [here](https://arxiv.org/pdf/1409.4842). This notebook is just a way for me to understand my code found in `googlenet.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ken zhang\\onedrive\\documents\\git\\machine-learning\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building GoogLeNet requires us to build 3 unqiue compoents:\n",
    "\n",
    "1. Custom convolutional layer\n",
    "2. The Inception layer\n",
    "3. Auxilary classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet's use of convolutional layer is the same as other networks. The reason it's \"unqiue\" is because every convolutional layer is followed by a rectified linear activation (ReLU). To avoid having to do write the pair everytime we want to use a convolutional layer, we will make a class that will do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding `**kwargs`, we keep the convolutionals as customizable as if we were just to call `nn.Conv2d()`. \n",
    "\n",
    "Performing batch normalization before ReLUs is common practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inception layer can be created by following the diagram in the papers. Architecture of individual branches can be constructed via Figure 2b and the individual details (kernel size, stride, etc) can be extracted from Table 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3_red, out_3x3, out_5x5_red, out_5x5, pool_proj):\n",
    "        super().__init__()\n",
    "        self.branch1 = Conv(in_channels, out_1x1, kernel_size=1, stride=1)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            Conv(in_channels, out_3x3_red, kernel_size=1, stride=1),\n",
    "            Conv(out_3x3_red, out_3x3, kernel_size=3, stride=1, padding=1), \n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            Conv(in_channels, out_5x5_red, kernel_size=1, stride=1),\n",
    "            Conv(out_5x5_red, out_5x5, kernel_size=5, stride=1, padding=2), \n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            Conv(in_channels, pool_proj, kernel_size=1, stride=1), \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        x = [branch1, branch2, branch3, branch4]\n",
    "        x = torch.cat(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each, branch's output are different, this module will reflect that by allowing us to indiviually assign the number of out channels for each branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxilary Classification Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This branch will force the module to predict early, allowing for benefits such as:\n",
    "\n",
    "1. Intermediate supervisions: model will receive gradient signals from these outputs as well as the final output\n",
    "2. Regularization: forces model to predict early on where there are less features\n",
    "3. Gradient flow: improves back propagation\n",
    "\n",
    "Details on how to build this layer can be located a few paragraphs below Table 1 where the paper highlights the architecture in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(5)\n",
    "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1, stride=1)\n",
    "        self.convR = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fcR = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.convR(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fcR(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GoogLeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all out components in place, we are now ready to build GoogLeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, aux, dropout, aux_dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxp1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.conv2 = Conv(64, 64, kernel_size=1, stride=1)\n",
    "        self.conv2a = Conv(64, 192, kernel_size=3, stride=1, padding=2)\n",
    "        self.maxp2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxp3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception(512, 160, 112, 224, 25, 64, 64)\n",
    "        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxp4 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.aux1 = AuxClassifier(512, num_classes, aux_dropout) if aux else None\n",
    "        self.aux2 = AuxClassifier(528, num_classes, aux_dropout) if aux else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxp1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.maxp2(x)\n",
    "\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxp3(x)\n",
    "\n",
    "        x = self.inception4a(x)\n",
    "\n",
    "        y = self.aux1(x) if self.aux1 and self.training else None\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "\n",
    "        z = self.aux2(x) if self.aux2 and self.training else None\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxp4(x)\n",
    "\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if self.aux1 and self.aux2 and self.training:\n",
    "            return x, y, z\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"fill in\" the parameters requires usage of both the table and the diagram. Things to note when using them is that\n",
    "\n",
    "**Diagram**\n",
    "\n",
    "Block notation of `nxn + s (S)` indicates the layer has \n",
    "\n",
    "- kernel of size `nxn`\n",
    "- stride of `s`\n",
    "- `S`AME or `V`ALID padding \n",
    "\n",
    "SAME has padding and VALID has no padding\n",
    "\n",
    "Padding can be calculated via:\n",
    "\n",
    "$Padding = [Kernel\\_Size - 1] / 2$\n",
    "\n",
    "**Table**\n",
    "\n",
    "Assume this in the output column...\n",
    "\n",
    "```\n",
    "14×14×480\n",
    "14×14×512\n",
    "14×14×512\n",
    "```\n",
    "\n",
    "...the layers coded will be something like...\n",
    "\n",
    "```\n",
    "Conv(in_channels, 480)\n",
    "Conv(480, 512)\n",
    "Conv(515, 513)\n",
    "```\n",
    "\n",
    "...since the number of channels for a layer's input will be the number of channel's in it's previous layer's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model by passing a dummy tensor of size `[2, 3, 244, 244]` which represents two 244x244 RGB images. We know the model works when this dummy tensor passes through the model without triggering an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    net = GoogLeNet(False, 0.4, 0.7, 1000)\n",
    "    x = torch.randn(10, 3, 224, 224)\n",
    "    y = net(x).to('cuda')\n",
    "    print(y.shape)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
